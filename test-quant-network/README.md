## About
A very simple network to test quantisation on khadas



## BUG
* found a bug in the images so everything i documented for the pyconvert case is probably wrong / tainted
* however for the shell script part it may work

## Notes 1
* it seems that on khadas no matter what configured and using the py convert tool all images
are changed to NCHW if they were read in as NHWC (so channel order is changed for some reason)
* the scaling is really strange it seems that image data

# Notes 2
Here is an example for 0.bmp
(This was with using --mean-values '127.5,127.5,127.5,127.5' and
 --quantized-dtype dynamic_fixed_point --qtype int8)

BEFORE running through network (the 1.5 have been added to create a similar output)
[  1.5   1.5   1.5   1.5   1.5   1.5   1.5   1.5   1.5   1.5   1.5   1.5
   1.5   1.5   1.5   1.5   1.5   1.5   1.5   1.5   1.5   1.5   1.5   1.5
   1.5   1.5   1.5   1.5   1.5   1.5   0.    0.    0.    0.    0.    0.
   0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
   0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
   0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
   0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
   0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
   0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
   0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
   0.    0.    0.    0.    0.    0.    0.   50.  159.  253.  159.   49.
   0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
   0.    0.    0.    0.    0.    0.    0.    0.    0.    0.   47.  238.
 252.  252.  252.  237.    0.    0.    0.    0.    0.    0.    0.    0.
   0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
   0.   53.  227.  253.  252.  239.  233.  252.   56.    5.    0.    0.
   0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
   0.    0.    0.    9.   59.  224.  252.  253.  252.  202.   84.  252.
 253.  122.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
   0.    0.    0.    0.    0.    0.    0.  163.  252.  252.  252.  253.
 252.  252.   96.  189.  253.  167.    0.    0.    0.    0.    0.    0.
   0.    0.    0.    0.    0.    0.    0.    0.    0.    0.   50.  238.
 253.  253.  190.  114.  253.  228.   46.   79.  255.  168.    0.    0.
   0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
   0.   47.  238.  252.  252.  179.   11.   75.  121.   20.    0.    0. ]


Taking the first non artificial element 50 we end up with 24.5
How did this happen?

Scaling as provided to convert tool
(50 - 127.5) / 127.5 == -0.6078 * 0.5 (nopenet) == -0.30392156862745096
Now rescale backwards (assuming this is done)
-0.30392156862745096 * 127.5 + 127.5 = 88.75

But it rather looks like: 50 * 0.5 = 25.0 but where does the -0.5 come from?
And it looks like there was no scaling at all or it was completely transparent;
this would explain why the networks values were not changed at all (the 1.5).


AFTER NETWORK
[  1.5   1.5   1.5   1.5   1.5   1.5   1.5   1.5   1.5   1.5   1.5   1.5
   1.5   1.5   1.5   1.5   1.5   1.5   1.5   1.5   1.5   1.5   1.5   1.5
   1.5   1.5   1.5   1.5   1.5   1.5   0.    0.    0.    0.    0.    0.
   0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
   0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
   0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
   0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
   0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
   0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
   0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
   0.    0.    0.    0.    0.    0.    0.   24.5  78.5 125.   78.5  24.
   0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
   0.    0.    0.    0.    0.    0.    0.    0.    0.    0.   23.  117.5
 124.5 124.5 124.5 117.    0.    0.    0.    0.    0.    0.    0.    0.
   0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
   0.   26.  112.  125.  124.5 118.  115.  124.5  27.5   2.5   0.    0.
   0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
   0.    0.    0.    4.5  29.  110.5 124.5 125.  124.5 100.   41.5 124.5
 125.   60.5   0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
   0.    0.    0.    0.    0.    0.    0.   80.5 124.5 124.5 124.5 125.
 124.5 124.5  47.5  93.5 125.   82.5   0.    0.    0.    0.    0.    0.
   0.    0.    0.    0.    0.    0.    0.    0.    0.    0.   24.5 117.5
 125.  125.   94.   56.5 125.  112.5  22.5  39.  126.   83.    0.    0.
   0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
   0.   23.  117.5 124.5 124.5  88.5   5.5  37.   60.   10.    0.    0. ]


## Notes 3
* uint8 is not working for ksnn conversion
* int8 with 
* --mean-values '0,0,0,1' \
--quantized-dtype dynamic_fixed_point \
--qtype int8 \

And this in the script
    ## test
    img2 = img.transpose(2,0,1).flatten().astype(np.int8).astype(float)
    img2[0:30] = 2.
    print(img2[0:300])


I was able to get a very similar output.
It shows that if no scaling is given the network simply casts everything
from the input to int8 (after transposing the channels).
We can observe that the values close to zero are correct while the other ones
further away from zero are off by 1. The offset sets in when a value is > 64.

Get input data ...
before network
[  2.   2.   2.   2.   2.   2.   2.   2.   2.   2.   2.   2.   2.   2.
   2.   2.   2.   2.   2.   2.   2.   2.   2.   2.   2.   2.   2.   2.
   2.   2.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.
   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.
   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.
   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.
   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.
   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.
   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.
   0.  50. -97.  -3. -97.  49.   0.   0.   0.   0.   0.   0.   0.   0.
   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.
  47. -18.  -4.  -4.  -4. -19.   0.   0.   0.   0.   0.   0.   0.   0.
   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  53.
 -29.  -3.  -4. -17. -23.  -4.  56.   5.   0.   0.   0.   0.   0.   0.
   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   9.  59. -32.
  -4.  -3.  -4. -54.  84.  -4.  -3. 122.   0.   0.   0.   0.   0.   0.
   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0. -93.  -4.  -4.
  -4.  -3.  -4.  -4.  96. -67.  -3. -89.   0.   0.   0.   0.   0.   0.
   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  50. -18.  -3.  -3.
 -66. 114.  -3. -28.  46.  79.  -1. -88.   0.   0.   0.   0.   0.   0.
   0.   0.   0.   0.   0.   0.   0.   0.   0.  47. -18.  -4.  -4. -77.
  11.  75. 121.  20.   0.   0.]
Done.
Start inference ...
Done. inference time:  0.0019216537475585938
[  2.   2.   2.   2.   2.   2.   2.   2.   2.   2.   2.   2.   2.   2.
   2.   2.   2.   2.   2.   2.   2.   2.   2.   2.   2.   2.   2.   2.
   2.   2.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.
   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.
   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.
   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.
   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.
   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.
   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.
   0.  50. -96.  -3. -96.  49.   0.   0.   0.   0.   0.   0.   0.   0.
   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.
  47. -18.  -4.  -4.  -4. -19.   0.   0.   0.   0.   0.   0.   0.   0.
   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  53.
 -29.  -3.  -4. -17. -23.  -4.  56.   5.   0.   0.   0.   0.   0.   0.
   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   9.  59. -32.
  -4.  -3.  -4. -54.  83.  -4.  -3. 121.   0.   0.   0.   0.   0.   0.
   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0. -92.  -4.  -4.
  -4.  -3.  -4.  -4.  95. -66.  -3. -88.   0.   0.   0.   0.   0.   0.
   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  50. -18.  -3.  -3.
 -65. 113.  -3. -28.  46.  78.  -1. -87.   0.   0.   0.   0.   0.   0.
   0.   0.   0.   0.   0.   0.   0.   0.   0.  47. -18.  -4.  -4. -76.




Here is the output for 65:

 |--- KSNN Version: v1.2 +---|
Start init neural network ...
Done.
Get input data ...
before network
[65. 65. 65. 65. 65. 65. 65. 65. 65. 65. 65. 65. 65. 65. 65. 65. 65. 65.
 65. 65. 65. 65. 65. 65. 65. 65. 65. 65. 65. 65. 65. 65. 65. 65. 65. 65.
 65. 65. 65. 65. 65. 65. 65. 65. 65. 65. 65. 65. 65. 65. 65. 65. 65. 65.
 65. 65. 65. 65. 65. 65. 65. 65. 65. 65. 65. 65. 65. 65. 65. 65. 65. 65.
 65. 65. 65. 65. 65. 65. 65. 65. 65. 65. 65. 65. 65. 65. 65. 65. 65. 65.
 65. 65. 65. 65. 65. 65. 65. 65. 65. 65. 65. 65. 65. 65. 65. 65. 65. 65.
 65. 65. 65. 65. 65. 65. 65. 65. 65. 65. 65. 65. 65. 65. 65. 65. 65. 65.
 65. 65. 65. 65. 65. 65. 65. 65. 65. 65. 65. 65. 65. 65. 65. 65. 65. 65.
 65. 65. 65. 65. 65. 65. 65. 65. 65. 65. 65. 65. 65. 65. 65. 65. 65. 65.
 65. 65. 65. 65. 65. 65. 65. 65. 65. 65. 65. 65. 65. 65. 65. 65. 65. 65.
 65. 65. 65. 65. 65. 65. 65. 65. 65. 65. 65. 65. 65. 65. 65. 65. 65. 65.
 65. 65. 65. 65. 65. 65. 65. 65. 65. 65. 65. 65. 65. 65. 65. 65. 65. 65.
 65. 65. 65. 65. 65. 65. 65. 65. 65. 65. 65. 65. 65. 65. 65. 65. 65. 65.
 65. 65. 65. 65. 65. 65. 65. 65. 65. 65. 65. 65. 65. 65. 65. 65. 65. 65.
 65. 65. 65. 65. 65. 65. 65. 65. 65. 65. 65. 65. 65. 65. 65. 65. 65. 65.
 65. 65. 65. 65. 65. 65. 65. 65. 65. 65. 65. 65. 65. 65. 65. 65. 65. 65.
 65. 65. 65. 65. 65. 65. 65. 65. 65. 65. 65. 65.]
Done.
Start inference ...
Done. inference time:  0.001972675323486328
[66. 66. 66. 66. 66. 66. 66. 66. 66. 66. 66. 66. 66. 66. 66. 66. 66. 66.
 66. 66. 66. 66. 66. 66. 66. 66. 66. 66. 66. 66. 64. 64. 64. 64. 64. 64.
 64. 64. 64. 64. 64. 64. 64. 64. 64. 64. 64. 64. 64. 64. 64. 64. 64. 64.
 64. 64. 64. 64. 64. 64. 64. 64. 64. 64. 64. 64. 64. 64. 64. 64. 64. 64.
 64. 64. 64. 64. 64. 64. 64. 64. 64. 64. 64. 64. 64. 64. 64. 64. 64. 64.
 64. 64. 64. 64. 64. 64. 64. 64. 64. 64. 64. 64. 64. 64. 64. 64. 64. 64.
 64. 64. 64. 64. 64. 64. 64. 64. 64. 64. 64. 64. 64. 64. 64. 64. 64. 64.
 64. 64. 64. 64. 64. 64. 64. 64. 64. 64. 64. 64. 64. 64. 64. 64. 64. 64.
 64. 64. 64. 64. 64. 64. 64. 64. 64. 64. 64. 64. 64. 64. 64. 64. 64. 64.
 64. 64. 64. 64. 64. 64. 64. 64. 64. 64. 64. 64. 64. 64. 64. 64. 64. 64.
 64. 64. 64. 64. 64. 64. 64. 64. 64. 64. 64. 64. 64. 64. 64. 64. 64. 64.
 64. 64. 64. 64. 64. 64. 64. 64. 64. 64. 64. 64. 64. 64. 64. 64. 64. 64.
 64. 64. 64. 64. 64. 64. 64. 64. 64. 64. 64. 64. 64. 64. 64. 64. 64. 64.
 64. 64. 64. 64. 64. 64. 64. 64. 64. 64. 64. 64. 64. 64. 64. 64. 64. 64.
 64. 64. 64. 64. 64. 64. 64. 64. 64. 64. 64. 64. 64. 64. 64. 64. 64. 64.
 64. 64. 64. 64. 64. 64. 64. 64. 64. 64. 64. 64. 64. 64. 64. 64. 64. 64.
 64. 64. 64. 64. 64. 64. 64. 64. 64. 64. 64. 64.]


I stopped trying to use the python converter - it seems there is just too much magic going on...