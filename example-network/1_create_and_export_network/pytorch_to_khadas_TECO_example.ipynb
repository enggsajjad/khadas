{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load libraries and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==>>> total trainning batch number: 300\n",
      "==>>> total testing batch number: 50\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import PIL\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "## load mnist dataset\n",
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "root = './data'\n",
    "if not os.path.exists(root):\n",
    "    os.mkdir(root)\n",
    "\n",
    "## added conversion for mnist to rgb\n",
    "## cf. https://discuss.pytorch.org/t/grayscale-to-rgb-transform/18315/5\n",
    "trans = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (1.0,)), transforms.Lambda(lambda x: x.repeat(3, 1, 1) )])\n",
    "\n",
    "width = 28\n",
    "height = 28\n",
    "\n",
    "# if not exist, download mnist dataset\n",
    "train_set = dset.MNIST(root=root, train=True, transform=trans, download=True)\n",
    "test_set = dset.MNIST(root=root, train=False, transform=trans, download=True)\n",
    "\n",
    "batch_size = 50\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "                 dataset=train_set,\n",
    "                 batch_size=batch_size,\n",
    "                 shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "                dataset=test_set,\n",
    "                batch_size=batch_size,\n",
    "                shuffle=False,)\n",
    "\n",
    "epochs = 15\n",
    "\n",
    "print('==>>> total trainning batch number: {}'.format(len(train_loader)))\n",
    "print('==>>> total testing batch number: {}'.format(len(test_loader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(use_cuda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## define lenet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LeNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 20, 5, 1)\n",
    "        self.conv2 = nn.Conv2d(20, 50, 5, 1)\n",
    "        self.fc1 = nn.Linear(4*4*50, 500)\n",
    "        self.fc2 = nn.Linear(500, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.reshape(-1,3,28,28)\n",
    "        x = x[:,0,:,:]\n",
    "        x = x.reshape(-1,1,28,28)\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = x.view(-1, 4*4*50)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "    \n",
    "    def name(self):\n",
    "        return \"LeNet\"\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export rgb images from the dataset for testing here and on khadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "0\n",
      "4\n",
      "1\n",
      "9\n",
      "2\n",
      "3\n",
      "6\n",
      "7\n",
      "8\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1407a443a60>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "numbers = []\n",
    "for i in range(len(train_set)):\n",
    "    image, label = train_set[i]\n",
    "    if label not in numbers:\n",
    "        print(label)\n",
    "        # denormalize\n",
    "        imag=(np.array(image.tolist())+0.5) * 255\n",
    "        #print(imag.shape)\n",
    "        # shape image from CHW -> HWC\n",
    "        imag = np.ascontiguousarray( imag.transpose((1,2,0)), dtype=np.uint8)\n",
    "        #print(imag.shape)\n",
    "        #print(imag.astype(np.uint8))\n",
    "        pil_image = PIL.Image.frombytes('RGB',(28,28), imag)\n",
    "        pil_image.save(str(label)+\".bmp\")\n",
    "        numbers.append(label)\n",
    "        \n",
    "        \n",
    "# visualize the last image as example\n",
    "plt.imshow(imag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train lenet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==>>> epoch: 0, batch index: 100, train loss: 0.580859\n",
      "==>>> epoch: 0, batch index: 200, train loss: 0.216005\n",
      "==>>> epoch: 0, batch index: 300, train loss: 0.163502\n",
      "==>>> epoch: 0, batch index: 50, test loss: 0.101111, acc: 0.961\n",
      "==>>> epoch: 1, batch index: 100, train loss: 0.115634\n",
      "==>>> epoch: 1, batch index: 200, train loss: 0.103781\n",
      "==>>> epoch: 1, batch index: 300, train loss: 0.076157\n",
      "==>>> epoch: 1, batch index: 50, test loss: 0.053621, acc: 0.979\n",
      "==>>> epoch: 2, batch index: 100, train loss: 0.084741\n",
      "==>>> epoch: 2, batch index: 200, train loss: 0.084439\n",
      "==>>> epoch: 2, batch index: 300, train loss: 0.068169\n",
      "==>>> epoch: 2, batch index: 50, test loss: 0.044117, acc: 0.980\n",
      "==>>> epoch: 3, batch index: 100, train loss: 0.053320\n",
      "==>>> epoch: 3, batch index: 200, train loss: 0.059875\n",
      "==>>> epoch: 3, batch index: 300, train loss: 0.052170\n",
      "==>>> epoch: 3, batch index: 50, test loss: 0.033682, acc: 0.983\n",
      "==>>> epoch: 4, batch index: 100, train loss: 0.048702\n",
      "==>>> epoch: 4, batch index: 200, train loss: 0.045587\n",
      "==>>> epoch: 4, batch index: 300, train loss: 0.034836\n",
      "==>>> epoch: 4, batch index: 50, test loss: 0.026058, acc: 0.987\n",
      "==>>> epoch: 5, batch index: 100, train loss: 0.035628\n",
      "==>>> epoch: 5, batch index: 200, train loss: 0.042158\n",
      "==>>> epoch: 5, batch index: 300, train loss: 0.044037\n",
      "==>>> epoch: 5, batch index: 50, test loss: 0.029282, acc: 0.986\n",
      "==>>> epoch: 6, batch index: 100, train loss: 0.041849\n",
      "==>>> epoch: 6, batch index: 200, train loss: 0.034720\n",
      "==>>> epoch: 6, batch index: 300, train loss: 0.027961\n",
      "==>>> epoch: 6, batch index: 50, test loss: 0.023803, acc: 0.989\n",
      "==>>> epoch: 7, batch index: 100, train loss: 0.030798\n",
      "==>>> epoch: 7, batch index: 200, train loss: 0.029748\n",
      "==>>> epoch: 7, batch index: 300, train loss: 0.034689\n",
      "==>>> epoch: 7, batch index: 50, test loss: 0.022514, acc: 0.986\n",
      "==>>> epoch: 8, batch index: 100, train loss: 0.029300\n",
      "==>>> epoch: 8, batch index: 200, train loss: 0.032237\n",
      "==>>> epoch: 8, batch index: 300, train loss: 0.036734\n",
      "==>>> epoch: 8, batch index: 50, test loss: 0.017998, acc: 0.990\n",
      "==>>> epoch: 9, batch index: 100, train loss: 0.015729\n",
      "==>>> epoch: 9, batch index: 200, train loss: 0.017917\n",
      "==>>> epoch: 9, batch index: 300, train loss: 0.020625\n",
      "==>>> epoch: 9, batch index: 50, test loss: 0.019872, acc: 0.989\n"
     ]
    }
   ],
   "source": [
    "## training\n",
    "model = LeNet()\n",
    "\n",
    "if use_cuda:\n",
    "    model = model.cuda()\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # trainning\n",
    "    ave_loss = 0\n",
    "    for batch_idx, (x, target) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if use_cuda:\n",
    "            x, target = x.cuda(), target.cuda()\n",
    "        else:\n",
    "            x, target = Variable(x), Variable(target)\n",
    "\n",
    "        out = model(x)\n",
    "        loss = criterion(out, target)\n",
    "        ave_loss = ave_loss * 0.9 + loss.item() * 0.1\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if (batch_idx+1) % 100 == 0 or (batch_idx+1) == len(train_loader):\n",
    "            print('==>>> epoch: {}, batch index: {}, train loss: {:.6f}'.format(\n",
    "                epoch, batch_idx+1, ave_loss))\n",
    "    # testing\n",
    "    correct_cnt, ave_loss = 0, 0\n",
    "    total_cnt = 0\n",
    "    for batch_idx, (x, target) in enumerate(test_loader):\n",
    "        with torch.no_grad():\n",
    "            if use_cuda:\n",
    "                x, target = x.cuda(), target.cuda()\n",
    "            else:\n",
    "                x, target = Variable(x), Variable(target)\n",
    "        out = model(x)\n",
    "        loss = criterion(out, target)\n",
    "        _, pred_label = torch.max(out.data, 1)\n",
    "        total_cnt += x.data.size()[0]\n",
    "        correct_cnt += (pred_label == target.data).sum()\n",
    "        # smooth average\n",
    "        ave_loss = ave_loss * 0.9 + loss.item() * 0.1\n",
    "        \n",
    "        if(batch_idx+1) % 100 == 0 or (batch_idx+1) == len(test_loader):\n",
    "            print('==>>> epoch: {}, batch index: {}, test loss: {:.6f}, acc: {:.3f}'.format(\n",
    "                epoch, batch_idx+1, ave_loss, correct_cnt.item() * 1.0 / total_cnt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save trained lenet as pyTorch native format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, \"lenet.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict using the stored model and previously exported example images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image is classified as 0 -> should be 0\n",
      "Image is classified as 1 -> should be 1\n",
      "Image is classified as 2 -> should be 2\n",
      "Image is classified as 3 -> should be 3\n",
      "Image is classified as 4 -> should be 4\n",
      "Image is classified as 5 -> should be 5\n",
      "Image is classified as 6 -> should be 6\n",
      "Image is classified as 7 -> should be 7\n",
      "Image is classified as 8 -> should be 8\n"
     ]
    }
   ],
   "source": [
    "import cv2 as cv\n",
    "\n",
    "## conv function represent different ways to implement the de-normalization\n",
    "def conv_fn1(data):\n",
    "    r_data = ((np.array(data) / 255.) - 0.5) / 1.0\n",
    "    return r_data\n",
    "\n",
    "\n",
    "def conv_fn2(data):\n",
    "    r_data = ((np.array(data) - 127.5) / 255.)\n",
    "    return r_data\n",
    "\n",
    "\n",
    "\n",
    "def test_acc(model, conv_fn, verb=False, transpose=True):\n",
    "    for i in range(0,9):\n",
    "        im = cv.imread(\"%s.bmp\"%i, cv.IMREAD_COLOR)\n",
    "        img = conv_fn(im)\n",
    "        if transpose: # HWC -> CHW\n",
    "             img = img.transpose((2,0,1))\n",
    "        img_tensor = torch.from_numpy(img).reshape(1,3,28,28).float()\n",
    "        if use_cuda:\n",
    "            model = model.cuda()\n",
    "            img_tensor = img_tensor.cuda()\n",
    "        out = model(img_tensor)\n",
    "        res = np.argmax( out.tolist()[0] )\n",
    "        print(\"Image is classified as %i -> should be %s\"%(res, i))\n",
    "        if verb:\n",
    "            print(np.array(out.tolist()[0]) * 127.5 + 127.5)\n",
    "    \n",
    "    \n",
    "    \n",
    "# predict using test image using opencv (like we would do on khadas)\n",
    "# load model\n",
    "l_model = torch.load(\"lenet.pt\")\n",
    "\n",
    "test_acc(l_model, conv_fn1, verb=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## export lmodel to onnx -> check output of export very carefully; dont miss any error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "graph(%input : Float(1, 3, 28, 28, strides=[2352, 784, 28, 1], requires_grad=0, device=cuda:0),\n",
      "      %conv1.bias : Float(20, strides=[1], requires_grad=0, device=cuda:0),\n",
      "      %conv1.weight : Float(20, 1, 5, 5, strides=[25, 25, 5, 1], requires_grad=0, device=cuda:0),\n",
      "      %conv2.bias : Float(50, strides=[1], requires_grad=0, device=cuda:0),\n",
      "      %conv2.weight : Float(50, 20, 5, 5, strides=[500, 25, 5, 1], requires_grad=0, device=cuda:0),\n",
      "      %fc1.bias : Float(500, strides=[1], requires_grad=0, device=cuda:0),\n",
      "      %fc1.weight : Float(500, 800, strides=[800, 1], requires_grad=0, device=cuda:0),\n",
      "      %fc2.bias : Float(10, strides=[1], requires_grad=0, device=cuda:0),\n",
      "      %fc2.weight : Float(10, 500, strides=[500, 1], requires_grad=0, device=cuda:0)):\n",
      "  %9 : Long(4, strides=[1], device=cpu) = onnx::Constant[value= -1   3  28  28 [ CPUDoubleType{4} ]]() # C:\\Users\\scholz\\AppData\\Local\\Temp\\ipykernel_3704\\836706537.py:10:0\n",
      "  %26 : Long(4, strides=[1], requires_grad=0, device=cpu) = onnx::Cast[to=7](%9) # C:\\Users\\scholz\\AppData\\Local\\Temp\\ipykernel_3704\\836706537.py:10:0\n",
      "  %x : Float(1, 3, 28, 28, strides=[2352, 784, 28, 1], device=cpu) = onnx::Reshape(%input, %26) # C:\\Users\\scholz\\AppData\\Local\\Temp\\ipykernel_3704\\836706537.py:10:0\n",
      "  %11 : Long(device=cpu) = onnx::Constant[value={0}]() # C:\\Users\\scholz\\AppData\\Local\\Temp\\ipykernel_3704\\836706537.py:11:0\n",
      "  %27 : Long(requires_grad=0, device=cpu) = onnx::Cast[to=7](%11) # C:\\Users\\scholz\\AppData\\Local\\Temp\\ipykernel_3704\\836706537.py:11:0\n",
      "  %12 : Float(1, 28, 28, strides=[784, 28, 1], device=cpu) = onnx::Gather[axis=1](%x, %27) # C:\\Users\\scholz\\AppData\\Local\\Temp\\ipykernel_3704\\836706537.py:11:0\n",
      "  %13 : Long(4, strides=[1], device=cpu) = onnx::Constant[value= -1   1  28  28 [ CPUDoubleType{4} ]]() # C:\\Users\\scholz\\AppData\\Local\\Temp\\ipykernel_3704\\836706537.py:12:0\n",
      "  %28 : Long(4, strides=[1], requires_grad=0, device=cpu) = onnx::Cast[to=7](%13) # C:\\Users\\scholz\\AppData\\Local\\Temp\\ipykernel_3704\\836706537.py:12:0\n",
      "  %input.1 : Float(1, 1, 28, 28, strides=[784, 784, 28, 1], device=cpu) = onnx::Reshape(%12, %28) # C:\\Users\\scholz\\AppData\\Local\\Temp\\ipykernel_3704\\836706537.py:12:0\n",
      "  %input.3 : Float(1, 20, 24, 24, strides=[11520, 576, 24, 1], device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[5, 5], pads=[0, 0, 0, 0], strides=[1, 1]](%input.1, %conv1.weight, %conv1.bias), scope: __module.conv1 # F:\\Documents\\Firma\\Projekte\\Metirionic\\code\\deus\\khadas\\example-network\\1_create_and_export_network\\venv\\lib\\site-packages\\torch\\nn\\modules\\conv.py:443:0\n",
      "  %input.7 : Float(1, 20, 24, 24, strides=[11520, 576, 24, 1], device=cpu) = onnx::Relu(%input.3) # F:\\Documents\\Firma\\Projekte\\Metirionic\\code\\deus\\khadas\\example-network\\1_create_and_export_network\\venv\\lib\\site-packages\\torch\\nn\\functional.py:1442:0\n",
      "  %input.11 : Float(1, 20, 12, 12, strides=[2880, 144, 12, 1], device=cpu) = onnx::MaxPool[kernel_shape=[2, 2], pads=[0, 0, 0, 0], strides=[2, 2]](%input.7) # F:\\Documents\\Firma\\Projekte\\Metirionic\\code\\deus\\khadas\\example-network\\1_create_and_export_network\\venv\\lib\\site-packages\\torch\\nn\\functional.py:797:0\n",
      "  %input.15 : Float(1, 50, 8, 8, strides=[3200, 64, 8, 1], device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[5, 5], pads=[0, 0, 0, 0], strides=[1, 1]](%input.11, %conv2.weight, %conv2.bias), scope: __module.conv2 # F:\\Documents\\Firma\\Projekte\\Metirionic\\code\\deus\\khadas\\example-network\\1_create_and_export_network\\venv\\lib\\site-packages\\torch\\nn\\modules\\conv.py:443:0\n",
      "  %input.19 : Float(1, 50, 8, 8, strides=[3200, 64, 8, 1], device=cpu) = onnx::Relu(%input.15) # F:\\Documents\\Firma\\Projekte\\Metirionic\\code\\deus\\khadas\\example-network\\1_create_and_export_network\\venv\\lib\\site-packages\\torch\\nn\\functional.py:1442:0\n",
      "  %x.4 : Float(1, 50, 4, 4, strides=[800, 16, 4, 1], device=cpu) = onnx::MaxPool[kernel_shape=[2, 2], pads=[0, 0, 0, 0], strides=[2, 2]](%input.19) # F:\\Documents\\Firma\\Projekte\\Metirionic\\code\\deus\\khadas\\example-network\\1_create_and_export_network\\venv\\lib\\site-packages\\torch\\nn\\functional.py:797:0\n",
      "  %21 : Long(2, strides=[1], device=cpu) = onnx::Constant[value=  -1  800 [ CPUDoubleType{2} ]]() # C:\\Users\\scholz\\AppData\\Local\\Temp\\ipykernel_3704\\836706537.py:17:0\n",
      "  %29 : Long(2, strides=[1], requires_grad=0, device=cpu) = onnx::Cast[to=7](%21) # C:\\Users\\scholz\\AppData\\Local\\Temp\\ipykernel_3704\\836706537.py:17:0\n",
      "  %input.23 : Float(1, 800, strides=[800, 1], device=cpu) = onnx::Reshape(%x.4, %29) # C:\\Users\\scholz\\AppData\\Local\\Temp\\ipykernel_3704\\836706537.py:17:0\n",
      "  %input.27 : Float(1, 500, strides=[500, 1], device=cpu) = onnx::Gemm[alpha=1., beta=1., transB=1](%input.23, %fc1.weight, %fc1.bias), scope: __module.fc1 # F:\\Documents\\Firma\\Projekte\\Metirionic\\code\\deus\\khadas\\example-network\\1_create_and_export_network\\venv\\lib\\site-packages\\torch\\nn\\modules\\linear.py:103:0\n",
      "  %input.31 : Float(1, 500, strides=[500, 1], device=cpu) = onnx::Relu(%input.27) # F:\\Documents\\Firma\\Projekte\\Metirionic\\code\\deus\\khadas\\example-network\\1_create_and_export_network\\venv\\lib\\site-packages\\torch\\nn\\functional.py:1442:0\n",
      "  %output : Float(1, 10, strides=[10, 1], requires_grad=1, device=cuda:0) = onnx::Gemm[alpha=1., beta=1., transB=1](%input.31, %fc2.weight, %fc2.bias), scope: __module.fc2 # F:\\Documents\\Firma\\Projekte\\Metirionic\\code\\deus\\khadas\\example-network\\1_create_and_export_network\\venv\\lib\\site-packages\\torch\\nn\\modules\\linear.py:103:0\n",
      "  return (%output)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def export_model_to_onnx(model):\n",
    "    input_dimension = torch.randn(1, 3, 28, 28)\n",
    "    if use_cuda:\n",
    "        input_dimension = input_dimension.cuda()\n",
    "    # very important or must leave out - not sure need to test again...\n",
    "    traced = torch.jit.trace(model, input_dimension)\n",
    "\n",
    "    torch.onnx.export(\n",
    "                      traced, \n",
    "                      input_dimension, \n",
    "                      \"lenet.onnx\",\n",
    "                      opset_version=7,\n",
    "                      verbose=True,\n",
    "                      export_params=True, \n",
    "                      input_names=['input'],\n",
    "                      output_names=['output'],\n",
    "                      dynamic_axes=None\n",
    "                      #dynamic_axes={'input': {0: 'batch', 2: 'height', 3: 'width'},  # shape(1,1,28,28)\n",
    "                      #              'output': {0: 'batch', 1: 'classes'} }  # shape(1,10)\n",
    "                      #example_outputs = torch_out\n",
    "    )\n",
    "\n",
    "export_model_to_onnx(l_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## check onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "graph torch-jit-export (\n",
      "  %input[FLOAT, 1x3x28x28]\n",
      ") optional inputs with matching initializers (\n",
      "  %conv1.bias[FLOAT, 20]\n",
      "  %conv1.weight[FLOAT, 20x1x5x5]\n",
      "  %conv2.bias[FLOAT, 50]\n",
      "  %conv2.weight[FLOAT, 50x20x5x5]\n",
      "  %fc1.bias[FLOAT, 500]\n",
      "  %fc1.weight[FLOAT, 500x800]\n",
      "  %fc2.bias[FLOAT, 10]\n",
      "  %fc2.weight[FLOAT, 10x500]\n",
      ") {\n",
      "  %onnx::Cast_9 = Constant[value = <Tensor>]()\n",
      "  %onnx::Reshape_26 = Cast[to = 7](%onnx::Cast_9)\n",
      "  %x = Reshape(%input, %onnx::Reshape_26)\n",
      "  %onnx::Cast_11 = Constant[value = <Scalar Tensor []>]()\n",
      "  %onnx::Gather_27 = Cast[to = 7](%onnx::Cast_11)\n",
      "  %onnx::Reshape_12 = Gather[axis = 1](%x, %onnx::Gather_27)\n",
      "  %onnx::Cast_13 = Constant[value = <Tensor>]()\n",
      "  %onnx::Reshape_28 = Cast[to = 7](%onnx::Cast_13)\n",
      "  %input.1 = Reshape(%onnx::Reshape_12, %onnx::Reshape_28)\n",
      "  %input.3 = Conv[dilations = [1, 1], group = 1, kernel_shape = [5, 5], pads = [0, 0, 0, 0], strides = [1, 1]](%input.1, %conv1.weight, %conv1.bias)\n",
      "  %input.7 = Relu(%input.3)\n",
      "  %input.11 = MaxPool[kernel_shape = [2, 2], pads = [0, 0, 0, 0], strides = [2, 2]](%input.7)\n",
      "  %input.15 = Conv[dilations = [1, 1], group = 1, kernel_shape = [5, 5], pads = [0, 0, 0, 0], strides = [1, 1]](%input.11, %conv2.weight, %conv2.bias)\n",
      "  %input.19 = Relu(%input.15)\n",
      "  %x.4 = MaxPool[kernel_shape = [2, 2], pads = [0, 0, 0, 0], strides = [2, 2]](%input.19)\n",
      "  %onnx::Cast_21 = Constant[value = <Tensor>]()\n",
      "  %onnx::Reshape_29 = Cast[to = 7](%onnx::Cast_21)\n",
      "  %input.23 = Reshape(%x.4, %onnx::Reshape_29)\n",
      "  %input.27 = Gemm[alpha = 1, beta = 1, transB = 1](%input.23, %fc1.weight, %fc1.bias)\n",
      "  %input.31 = Relu(%input.27)\n",
      "  %output = Gemm[alpha = 1, beta = 1, transB = 1](%input.31, %fc2.weight, %fc2.bias)\n",
      "  return %output\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import onnx\n",
    "\n",
    "# Load the ONNX model\n",
    "onnx_model = onnx.load(\"lenet.onnx\")\n",
    "\n",
    "# Check that the IR is well formed\n",
    "onnx.checker.check_model(onnx_model)\n",
    "\n",
    "# Print a Human readable representation of the graph\n",
    "print( onnx.helper.printable_graph(onnx_model.graph) )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39-teconet",
   "language": "python",
   "name": "py39-teconet"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
